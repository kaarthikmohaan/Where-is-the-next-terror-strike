# -*- coding: utf-8 -*-
"""cap_proj1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EOYVigLru_m8IbgEuHAphBljBrhiWynV

<p><img alt="Colaboratory logo" height="45px" src="/img/colab_favicon.ico" align="left" hspace="10px" vspace="0px"></p>

<h1>What is Colaboratory?</h1>

Colaboratory, or "Colab" for short, allows you to write and execute Python in your browser, with 
- Zero configuration required
- Free access to GPUs
- Easy sharing

Whether you're a **student**, a **data scientist** or an **AI researcher**, Colab can make your work easier. Watch [Introduction to Colab](https://www.youtube.com/watch?v=inN8seMm7UI) to learn more, or just get started below!

## **Getting started**

The document you are reading is not a static web page, but an interactive environment called a **Colab notebook** that lets you write and execute code.

For example, here is a **code cell** with a short Python script that computes a value, stores it in a variable, and prints the result:
"""

seconds_in_a_day = 24 * 60 * 60
seconds_in_a_day

"""To execute the code in the above cell, select it with a click and then either press the play button to the left of the code, or use the keyboard shortcut "Command/Ctrl+Enter". To edit the code, just click the cell and start editing.

Variables that you define in one cell can later be used in other cells:
"""

seconds_in_a_week = 7 * seconds_in_a_day
seconds_in_a_week

"""Colab notebooks allow you to combine **executable code** and **rich text** in a single document, along with **images**, **HTML**, **LaTeX** and more. When you create your own Colab notebooks, they are stored in your Google Drive account. You can easily share your Colab notebooks with co-workers or friends, allowing them to comment on your notebooks or even edit them. To learn more, see [Overview of Colab](/notebooks/basic_features_overview.ipynb). To create a new Colab notebook you can use the File menu above, or use the following link: [create a new Colab notebook](http://colab.research.google.com#create=true).

Colab notebooks are Jupyter notebooks that are hosted by Colab. To learn more about the Jupyter project, see [jupyter.org](https://www.jupyter.org).

## Data science

With Colab you can harness the full power of popular Python libraries to analyze and visualize data. The code cell below uses **numpy** to generate some random data, and uses **matplotlib** to visualize it. To edit the code, just click the cell and start editing.
"""

import numpy as np
from matplotlib import pyplot as plt

ys = 200 + np.random.randn(100)
x = [x for x in range(len(ys))]

plt.plot(x, ys, '-')
plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)

plt.title("Sample Visualization")
plt.show()

"""You can import your own data into Colab notebooks from your Google Drive account, including from spreadsheets, as well as from Github and many other sources. To learn more about importing data, and how Colab can be used for data science, see the links below under [Working with Data](#working-with-data).

## Machine learning

With Colab you can import an image dataset, train an image classifier on it, and evaluate the model, all in just [a few lines of code](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/beginner.ipynb). Colab notebooks execute code on Google's cloud servers, meaning you can leverage the power of Google hardware, including [GPUs and TPUs](#using-accelerated-hardware), regardless of the power of your machine. All you need is a browser.

Colab is used extensively in the machine learning community with applications including:
- Getting started with TensorFlow
- Developing and training neural networks
- Experimenting with TPUs
- Disseminating AI research
- Creating tutorials

To see sample Colab notebooks that demonstrate machine learning applications, see the [machine learning examples](#machine-learning-examples) below.

## More Resources

### Working with Notebooks in Colab
- [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb)
- [Guide to Markdown](/notebooks/markdown_guide.ipynb)
- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)
- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)
- [Interactive forms](/notebooks/forms.ipynb)
- [Interactive widgets](/notebooks/widgets.ipynb)
- <img src="/img/new.png" height="20px" align="left" hspace="4px" alt="New"></img>
 [TensorFlow 2 in Colab](/notebooks/tensorflow_version.ipynb)

<a name="working-with-data"></a>
### Working with Data
- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb) 
- [Charts: visualizing data](/notebooks/charts.ipynb)
- [Getting started with BigQuery](/notebooks/bigquery.ipynb)

### Machine Learning Crash Course
These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.
- [Intro to Pandas](/notebooks/mlcc/intro_to_pandas.ipynb)
- [Tensorflow concepts](/notebooks/mlcc/tensorflow_programming_concepts.ipynb)
- [First steps with TensorFlow](/notebooks/mlcc/first_steps_with_tensor_flow.ipynb)
- [Intro to neural nets](/notebooks/mlcc/intro_to_neural_nets.ipynb)
- [Intro to sparse data and embeddings](/notebooks/mlcc/intro_to_sparse_data_and_embeddings.ipynb)

<a name="using-accelerated-hardware"></a>
### Using Accelerated Hardware
- [TensorFlow with GPUs](/notebooks/gpu.ipynb)
- [TensorFlow with TPUs](/notebooks/tpu.ipynb)

<a name="machine-learning-examples"></a>

## Machine Learning Examples

To see end-to-end examples of the interactive machine learning analyses that Colaboratory makes possible, check out these  tutorials using models from [TensorFlow Hub](https://tfhub.dev).

A few featured examples:

- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.
- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.
- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.
- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.
- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.

Installing libraries
"""

!apt-get install -q libgeos-3.5.0
!apt-get install -q libgeos-dev
!pip install -q https://github.com/matplotlib/basemap/archive/master.zip
!pip install -q pyproj==1.9.6

"""Importing python modules

"""

import pandas as pd #creation of dataframe from data set
import numpy as np
import networkx as nx #Network related tool in the python
import matplotlib.pyplot as plt #ploting and saving the image
from mpl_toolkits.basemap import Basemap as Basemap
from matplotlib.pyplot import figure
import itertools

#Mount the drive to colab
from google.colab import drive
drive.mount('/content/drive')

# Data set from https://gtd.terrorismdata.com/app/uploads/_mediavault/2021/02/globalterrorismdb_0221dist.xlsx
# Reading the data set and strore it in the variable called df 
df = pd.read_excel (r'/content/drive/MyDrive/capston/Copy of globalterrorismdb_0919dist.xlsx')
#df = pd.read_excel (r'/content/globalterrorismdb_0919dist.xlsx')

# About the data set
print("There are ",len(df),"rows and ",len(df.columns),"columns int the data set")
df.head(5)

"""Pre-processing"""

print("Number of rows in the data set before pre-processing",len(df))
df1 = df.dropna(subset=["country_txt","gname"])# Removing rows contains Nan in a specified column=["country_txt","gname"]
print("Number of rows in the data set after removal of null values",len(df1))

l=len(df)
df2=df
df=df.loc[df["doubtterr"]!=1]
print(l-len(df),"rows are removed")
print("Current rown is data set is",len(df))

"""Visualisation"""

#fetching latitude and logitude of the country
pos={}
lt=[]
lg=[]
cnt=[]
m = Basemap()
for i in df.index:
  c=df['country_txt'][i]
  if c not in cnt :
    cnt.append(c)
    if not (np.isnan(df["latitude"][i]) or np.isnan(df["longitude"][i])):
      lt.append(int(df["latitude"][i]))
      lg.append(int(df["longitude"][i]))
    else:
      lt.append(0)
      lg.append(0)
mx,my=m(lg,lt)
for i,j in enumerate(cnt):
  pos[j]=(mx[i],my[i])

#calculating the terror groups position using its target country nodes
calc={}
for i in df.index:
  target=df["country_txt"][i]
  source=df["gname"][i]
  if source in calc:
    lat,lng=pos[target]
    calc[source][0]+=lat
    calc[source][1]+=lng
    calc[source][2]+=1
  else:
    calc[source]=[0,0,0]
for i in calc:
  pos[i]=(0 if not calc[i][2] else calc[i][0]//calc[i][2],0 if not calc[i][2] else calc[i][1]//calc[i][2])

#Building the graph
G=nx.Graph()
for i in df.index:
  #print(i)
  target=df["country_txt"][i]
  source=df["gname"][i]
  #if not (math.isnan(target) or math.isnan(source)):
  G.add_edge(*(source,target))

#Ploting the graph
plt.figure(figsize=(20,10))
nx.draw_networkx(G,pos,node_size=20,node_color='blue',with_labels=False,edge_color=('r') )
# Now draw the map
m.drawcountries()
m.drawstates()
m.bluemarble()
plt.title('GTD')
#figure(figsize=(80, 60), dpi=800)
plt.savefig('plot.png')
plt.show()

print("There are ",len(set(df['country_txt'])),"countries and",len(set(df['gname'])),"terrorist groups")

[i for i in sorted(set(df['country_txt']))]#country list

#Building the network with 'india' as a target node
G=nx.Graph()
for i in df.index:
  #print(i)
  target=df["country_txt"][i]
  source=df["gname"][i]
  #if not (math.isnan(target) or math.isnan(source)):
  if target=='India' :
    G.add_edge(*(source,target))

plt.figure(figsize=(20,10))
nx.draw_networkx(G,pos,node_size=20,node_color='blue',with_labels=False,edge_color=('r') )
# Now draw the map
m.drawcountries()
m.drawstates()
m.bluemarble()
plt.title('GTD')
#figure(figsize=(80, 60), dpi=800)
plt.savefig('plot.png')
plt.show()

# Building the network of big terror groups(maximum attack)
calc={}
for i in df.index:
  target=df["country_txt"][i]
  source=df["gname"][i]
  if source in calc:
    calc[source].append(target)
  else:
    calc[source]=[target]
for i in calc:
  calc[i]=list(set(calc[i]))

c=sorted(calc, key=lambda item:len(calc.get(item)),reverse=True)
print(c[:4])

G=nx.Graph()
for i in calc[c[0]]:
  G.add_edge(*(c[0],i))

plt.figure(figsize=(20,10))
nx.draw_networkx(G,pos,node_size=20,node_color='blue',with_labels=False,edge_color=('r','b') )
# Now draw the map
m.drawcountries()
m.drawstates()
m.bluemarble()
plt.title('GTD')
#figure(figsize=(80, 60), dpi=800)
plt.savefig('plot.png')
plt.show()

#convert names of target and source to unique id
name={}
i_name={}
for j,i in enumerate(set(df["country_txt"]).union(set(df["gname"]))):
  name[i]=j
  i_name[j]=i

import networkx as nx
import matplotlib.pyplot as plt
import math
G=nx.Graph()
G.add_nodes_from(list(set(df["country_txt"]).union(set(df["gname"])))) 
for i in df.index:
  #print(i)
  target=df["country_txt"][i]
  source=df["gname"][i]
  #if not (math.isnan(target) or math.isnan(source)):
  G.add_edge(*(source,target))
plt.figure(figsize =(18.97, 10.66))
nx.draw(G,node_size=3)
#plt.figure(figsize =(18.97, 10.66))
plt.savefig("simple_path.png") # save as png
plt.show()

set(df['iyear'])

#Buid the network based on year say 1970
import networkx as nx
import matplotlib.pyplot as plt
import math
G=nx.Graph()
#G.add_nodes_from(list(set(df["country_txt"]).union(set(df["gname"])))) 
for i in df.index:
  #print(i)
  if df['iyear'][i]>=1970 and df['iyear'][i]<=1970:
    target=df["country_txt"][i]
    source=df["gname"][i]
  #if not (math.isnan(target) or math.isnan(source)):
    G.add_edge(*(name[source],name[target]))

nx.draw(G,node_size=30)
#plt.figure(figsize =(18.97, 10.66))
plt.savefig("simple_path.png") # save as png
plt.show()

#Buid the network based on year say 1970 to 1980
import networkx as nx
import matplotlib.pyplot as plt
import math
G=nx.Graph()
#G.add_nodes_from(list(set(df["country_txt"]).union(set(df["gname"])))) 
for i in df.index:
  #print(i)
  if df['iyear'][i]>=1970 and df['iyear'][i]<=1980:
    target=df["country_txt"][i]
    source=df["gname"][i]
  #if not (math.isnan(target) or math.isnan(source)):
    G.add_edge(*(source,target))
plt.figure(figsize=(20,10))
nx.draw(G,node_size=10)
#plt.figure(figsize =(18.97, 10.66))
plt.savefig("simple_path.png") # save as png
plt.show()



"""Implimentation --Disparity filter algorithm"""

import numpy as np
dct={}
for i in df.index:
  if df['iyear'][i]>=1970 and df['iyear'][i]<=2020:
    tp=(df["gname"][i],df["country_txt"][i])
    kill=df['nkill'][i]
    wnd=df['nwound'][i]
    prop=df['propextent'][i]
    #target=df["country_txt"][i]
    #source=df["gname"][i]
    if tp not in dct:
      dct[tp]=[1,1,1]
    if  not np.isnan(kill):
        dct[tp][0]+=3*kill
    if  not np.isnan(wnd):
        dct[tp][1]+=0.5*wnd
    if  not np.isnan(prop):
        dct[tp][2]+=abs(4-prop)

import networkx as nx
G = nx.DiGraph()
for i in dct:
  G.add_edge(name[i[0]], name[i[1]], weight=int(sum(dct[i])))
  if int(sum(dct[i]))<0:
    print(dct[i])

G.number_of_nodes()

len(dct)

#!/usr/bin/env python
# encoding: utf-8

from networkx.readwrite import json_graph
from scipy.stats import percentileofscore
from traceback import format_exception
import cProfile
import json
import networkx as nx
import numpy as np
import pandas as pd
import pstats
import random
import sys
import matplotlib.pyplot as plt
DEBUG = False # True


######################################################################
## disparity filter for extracting the multiscale backbone of
## complex weighted networks

def get_nes (graph, label):
    """
    find the neighborhood attention set (NES) for the given label
    """
    for node_id in graph.nodes():
        node = graph.node[node_id]

        if node["label"].lower() == label:
            return set([node_id]).union(set([id for id in graph.neighbors(node_id)]))


def disparity_integral (x, k):
    """
    calculate the definite integral for the PDF in the disparity filter
    """
    assert x != 1.0, "x == 1.0"
    assert k != 1.0, "k == 1.0"
    return ((1.0 - x)**k) / ((k - 1.0) * (x - 1.0))


def get_disparity_significance (norm_weight, degree):
    """
    calculate the significance (alpha) for the disparity filter
    """
    return 1.0 - ((degree - 1.0) * (disparity_integral(norm_weight, degree) - disparity_integral(0.0, degree)))


def disparity_filter (graph):
    """
    implements a disparity filter, based on multiscale backbone networks
    https://arxiv.org/pdf/0904.2389.pdf
    """
    alpha_measures = []
    
    for node_id in graph.nodes():
        node = graph.nodes[node_id]
        degree = graph.degree(node_id)
        strength = 0.0
        

        for id0, id1 in graph.edges(nbunch=[node_id]):
            edge = graph[id0][id1]
            strength += edge["weight"]

        node["strength"] = strength

        for id0, id1 in graph.edges(nbunch=[node_id]):
            edge = graph[id0][id1]

            norm_weight = edge["weight"] / strength
            edge["norm_weight"] = norm_weight

            if degree > 1:
                try:
                    if norm_weight == 1.0:
                        norm_weight -= 0.0001

                    alpha = get_disparity_significance(norm_weight, degree)
                except AssertionError:
                    report_error("disparity {}".format(repr(node)), fatal=True)

                edge["alpha"] = alpha
                alpha_measures.append(alpha)
            else:
                edge["alpha"] = 0.0

    for id0, id1 in graph.edges():
        edge = graph[id0][id1]
        edge["alpha_ptile"] = percentileofscore(alpha_measures, edge["alpha"]) / 100.0

    return alpha_measures


######################################################################
## related metrics

def calc_centrality (graph, min_degree=1):
    """
    to conserve compute costs, ignore centrality for nodes below `min_degree`
    """
    sub_graph = graph.copy()
    sub_graph.remove_nodes_from([ n for n, d in list(graph.degree) if d < min_degree ])

    centrality = nx.betweenness_centrality(sub_graph, weight="weight")
    #centrality = nx.closeness_centrality(sub_graph, distance="distance")

    return centrality


def calc_quantiles (metrics, num):
    """
    calculate `num` quantiles for the given list
    """
    global DEBUG

    bins = np.linspace(0, 1, num=num, endpoint=True)
    s = pd.Series(metrics)
    q = s.quantile(bins, interpolation="nearest")

    try:
        dig = np.digitize(metrics, q) - 1
    except ValueError as e:
        print("ValueError:", str(e), metrics, s, q, bins)
        sys.exit(-1)

    quantiles = []

    for idx, q_hi in q.iteritems():
        quantiles.append(q_hi)

        if DEBUG:
            print(idx, q_hi)

    return quantiles


def calc_alpha_ptile (alpha_measures, show=True):
    """
    calculate the quantiles used to define a threshold alpha cutoff
    """
    quantiles = calc_quantiles(alpha_measures, num=10)
    num_quant = len(quantiles)

    if show:
        print("\tptile\talpha")

        for i in range(num_quant):
            percentile = i / float(num_quant)
            print("\t{:0.2f}\t{:0.4f}".format(percentile, quantiles[i]))

    return quantiles, num_quant


def cut_graph (graph, min_alpha_ptile=0.5, min_degree=2):
    """
    apply the disparity filter to cut the given graph
    """
    filtered_set = set([])

    for id0, id1 in graph.edges():
        edge = graph[id0][id1]

        if edge["alpha_ptile"] < min_alpha_ptile:
            filtered_set.add((id0, id1))

    for id0, id1 in filtered_set:
        graph.remove_edge(id0, id1)

    filtered_set = set([])

    for node_id in graph.nodes():
        node = graph.nodes[node_id]

        if graph.degree(node_id) < min_degree:
            filtered_set.add(node_id)

    for node_id in filtered_set:
        graph.remove_node(node_id)



######################################################################
## profiling utilities

def start_profiling ():
    """start profiling"""
    pr = cProfile.Profile()
    pr.enable()

    return pr


def stop_profiling (pr):
    """stop profiling and report"""
    pr.disable()

    s = io.StringIO()
    sortby = "cumulative"
    ps = pstats.Stats(pr, stream=s).sort_stats(sortby)

    ps.print_stats()
    print(s.getvalue())


def report_error (cause_string, logger=None, fatal=False):
    """
    TODO: errors should go to logger, and not be fatal
    """
    etype, value, tb = sys.exc_info()
    error_str = "{} {}".format(cause_string, str(format_exception(etype, value, tb, 3)))

    if logger:
        logger.info(error_str)
    else:
        print(error_str)

    if fatal:
        sys.exit(-1)


######################################################################
## graph serialization

def load_graph (graph_path):
    """
    load a graph from JSON
    """
    with open(graph_path) as f:
        data = json.load(f)
        graph = json_graph.node_link_graph(data, directed=True)
        return graph


def save_graph (graph, graph_path):
    """
    save a graph as JSON
    """
    with open(graph_path, "w") as f:
        data = json_graph.node_link_data(graph)
        json.dump(data, f)


######################################################################
## testing




def describe_graph (graph, min_degree=1, show_centrality=False):
    """
    describe a graph
    """
    print("\ngraph: {} nodes {} edges\n".format(len(graph.nodes()), len(graph.edges())))

    if show_centrality:
        print(calc_centrality(graph, min_degree))


def main (n=100, k=10, min_alpha_ptile=0.5, min_degree=2):
    # generate a random graph (from seed, always the same)
    #graph = random_graph(n, k)
    graph=G
    plt.figure(figsize=(20,10))
    nx.draw(graph,with_labels = True)
    plt.savefig("simple_path3.png") # save as png
    plt.show()

    save_graph(graph, "g.json")
    describe_graph(graph, min_degree)

    # calculate the multiscale backbone metrics
    alpha_measures = disparity_filter(graph)
    quantiles, num_quant = calc_alpha_ptile(alpha_measures)
    alpha_cutoff = quantiles[round(num_quant * min_alpha_ptile)]

    print("\nfilter: percentile {:0.2f}, min alpha {:0.4f}, min degree {}".format(
            min_alpha_ptile, alpha_cutoff, min_degree
            ))

    # apply the filter to cut the graph
    cut_graph(graph, min_alpha_ptile, min_degree)

    save_graph(graph, "h.json")
    describe_graph(graph, min_degree)
    plt.figure(figsize=(20,10))
    nx.draw(graph,with_labels = True)
    plt.savefig("simple_path4.png") # save as png
    plt.show()
######################################################################
## main entry point

if __name__ == "__main__":
    main()



for i,j in G.edges():
  print(i_name[i],"-------",i_name[j])

import networkx as nx
graph = nx.DiGraph()
edg=G.edges()
for i in dct:
  if (name[i[0]], name[i[1]]) in edg:
    
    graph.add_edge(name[i[0]], name[i[1]],color='r', weight=int(sum(dct[i])))
  else:
    graph.add_edge(name[i[0]], name[i[1]],color='g', weight=int(sum(dct[i])))
  if int(sum(dct[i]))<0:
    print(dct[i])
colors = nx.get_edge_attributes(graph,'color').values()
weights = nx.get_edge_attributes(graph,'weight').values()
plt.figure(figsize=(20,10))
#pos = nx.circular_layout(G)
nx.draw(graph, 
        edge_color=colors, 
        
        with_labels=True,
        )

#nx.draw(graph,with_labels = True)

out_degree=dict(graph.out_degree())
for i in sorted(out_degree,key=lambda x:out_degree[x],reverse=True)[0:10]:
  print(i_name[i])

in_degree=dict(graph.in_degree())
for i in sorted(in_degree,key=lambda x:in_degree[x],reverse=True)[0:10]:
  print(i_name[i])

for i in range(1980,2020,10):
  dct={}
  for i in df.index:
    if df['iyear'][i]>=i-10 and df['iyear'][i]<=i:
      tp=(df["gname"][i],df["country_txt"][i])
      kill=df['nkill'][i]
      wnd=df['nwound'][i]
      prop=df['propextent'][i]
      #target=df["country_txt"][i]
      #source=df["gname"][i]
      if tp not in dct:
        dct[tp]=[1,1,1]
      if  not np.isnan(kill):
          dct[tp][0]+=3*kill
      if  not np.isnan(wnd):
          dct[tp][1]+=0.5*wnd
      if  not np.isnan(prop):
          dct[tp][2]+=abs(4-prop)
  G = nx.DiGraph()
  for i in dct:
    G.add_edge(name[i[0]], name[i[1]], weight=int(sum(dct[i])))
    if int(sum(dct[i]))<0:
      print(dct[i])
  main()
  for i,j in G.edges():
    print(i_name[i],"-------",i_name[j])
  graph = nx.DiGraph()
  edg=G.edges()
  for i in dct:
    if (name[i[0]], name[i[1]]) in edg:
      
      graph.add_edge(name[i[0]], name[i[1]],color='r', weight=int(sum(dct[i])))
    else:
      graph.add_edge(name[i[0]], name[i[1]],color='g', weight=int(sum(dct[i])))
    if int(sum(dct[i]))<0:
      print(dct[i])
  colors = nx.get_edge_attributes(graph,'color').values()
  weights = nx.get_edge_attributes(graph,'weight').values()
  plt.figure(figsize=(20,10))
  #pos = nx.circular_layout(G)
  nx.draw(graph, 
          edge_color=colors, 
          
          with_labels=True,
          )

df



"""R2"""

import networkx as nx
Graph = nx.DiGraph()
for i in dct:
  Graph.add_edge(name[i[0]], name[i[1]], weight=int(sum(dct[i])))
  if int(sum(dct[i]))<0:
    print(dct[i])

#calling disparity filter algorithm
def mn (graph,n=100, k=10, min_alpha_ptile=0.5, min_degree=2):
    graph=graph.copy()
    save_graph(graph, "g.json")
    alpha_measures = disparity_filter(graph)
    cut_graph(graph, min_alpha_ptile, min_degree)
    return graph.number_of_nodes()

def plot_graph(G):
  x=[i/10 for i in range(0,10)]
  y=[mn(G,min_alpha_ptile=i/10) for i in range(0,10)]
  plt.title("Nodes selecetd v/s Alpha value")
  plt.xlabel("Alpha values")
  plt.ylabel("Nodes")
  plt.plot(x,y)
plot_graph(Graph)

#Directed graph to undirected graph
def convertGraph(G):
  graph=nx.Graph()
  for i in G.nodes:
    pred=[j for j in G.predecessors(i)]
    if len(pred):
      for j in list(itertools.combinations(pred,2)):
        graph.add_edge(j[0],j[1],weight=G[j[0]][i]['weight']+G[j[1]][i]['weight'])
    del pred
  return graph
graph=convertGraph(G)

def draw(G):
  plt.figure(figsize=(20,10))
  nx.draw_networkx(G, with_labels = False)
draw(graph)

#In the diffusion algorithm we use the priority queue such that the top most priority node is always present at the
#top of the queue
import itertools
from heapq import *
'''In Python, it is available using “heapq” module. 
The property of this data structure in Python is that each time the smallest of heap element is popped(min heap). 
Whenever elements are pushed or popped, heap structure in maintained.
The heap[0] element also returns the smallest element each time.'''
class PQ(object):
    def __init__(self):
        self.pq = []                         # list of entries arranged in a heap
        self.entry_finder = {}               # mapping of tasks to entries
        self.REMOVED = '<removed-task>'      # placeholder for a removed task
        self.counter = itertools.count()     # unique sequence count

    def add_task(self, task, priority=0):
        'Add a new task or update the priority of an existing task'
        if task in self.entry_finder:
            self.remove_task(task)
        count = next(self.counter)
        entry = [priority, count, task]
        self.entry_finder[task] = entry
        heappush(self.pq, entry)

    def remove_task(self, task):
        'Mark an existing task as REMOVED.  Raise KeyError if not found.'
        entry = self.entry_finder.pop(task)
        entry[-1] = self.REMOVED

    def pop_item(self):
        'Remove and return the lowest priority task. Raise KeyError if empty.'
        while self.pq:
            priority, count, task = heappop(self.pq)
            if task is not self.REMOVED:
                del self.entry_finder[task]
                return task, priority
        raise KeyError('pop from an empty priority queue')

    def __str__(self):
        return str([entry for entry in self.pq if entry[2] != self.REMOVED])

import networkx as nx

def degreeDiscountIC(G, k, p=.01):
    ''' Finds initial set of nodes to propagate in Independent Cascade model (with priority queue)
    Input: G -- networkx graph object
    k -- number of nodes needed
    p -- propagation probability
    Output:
    S -- chosen k nodes
    '''
    S = []
    dd = PQ() # degree discount
    t = dict() # number of adjacent vertices that are in S
    d = dict() # degree of each vertex

    # initialize degree discount
    
    for u in G.nodes():
        d[u] = sum([G[u][v]['weight'] for v in G[u]]) # each edge adds degree 1
        # d[u] = len(G[u]) # each neighbor adds degree 1
        dd.add_task(u, -d[u]) # add degree of each node
        t[u] = 0

    # add vertices to S greedily
    for i in range(k):
      u , priority = dd.pop_item() # extract node with maximal degree discount
      S.append(u)
        
      for v in G[u]:
        if v not in S:
                t[v] += G[u][v]['weight'] # increase number of selected neighbors
                #priority=d[v] - t[v]
                priority = d[v] - 2*t[v] - (d[v] - t[v])*t[v]*p # discount of degree
                #print("Priority",v,d[v],priority)
                dd.add_task(v, -priority)
    return S

def degreeDiscountIC2(G, k, p=.01):
     d = dict()
     dd = dict() # degree discount
     t = dict() # number of selected neighbors
     S = [] # selected set of nodes
     for u in G:
         d[u] = sum([G[u][v]['weight'] for v in G[u]]) # each edge adds degree 1
         # d[u] = len(G[u]) # each neighbor adds degree 1
         dd[u] = d[u]
         t[u] = 0
     for i in range(k):
         u, ddv = max(dd.items(), key=lambda v: v[1])
         dd.pop(u)
         S.append(u)
         for v in G[u]:
             if v not in S:
                 t[v] += G[u][v]['weight'] # increase number of selected neighbors
                 dd[v] = d[v] - 2*t[v] - (d[v] - t[v])*t[v]*p
     return S
    
def degreeDiscountStar(G,k,p=.01):
   
     S = []
     scores = PQ()
     d = dict()
     t = dict()
     for u in G:
         d[u] = sum([G[u][v]['weight'] for v in G[u]])
         t[u] = 0
         score = -((1-p)**t[u])*(1+(d[u]-t[u])*p)
         scores.add_task(u, )
     for iteration in range(k):
         u, priority = scores.pop_item()
         #print(iteration, -priority)
         S.append(u)
         for v in G[u]:
             if v not in S:
                 t[v] += G[u][v]['weight']
                 score = -((1-p)**t[u])*(1+(d[u]-t[u])*p)
                 scores.add_task(v, score)
     return S

def viewPriorityNode(G,nodes):
  pos = nx.spring_layout(G)
  plt.figure(figsize=(20,10))
# draw graph
  nx.draw_networkx(G, pos=pos, font_size=16, node_color='blue', font_color='white', with_labels = False)

# draw subgraph for highlights
  nx.draw_networkx(G.subgraph(nodes), pos=pos, font_size=16, node_color='red', font_color='green', with_labels = False)

#Testing
edges = [['A','B'], ['A','C'], ['A','D'], ['B','E'], ['B','F'], ['D','G'],['D','H'],['F','I'],['G','J'],['A','K']]
G = nx.Graph()
G.add_edges_from(edges)
viewPriorityNode(G,['A','C'])

#Testing
import networkx as nx
G = nx.Graph()
  
edges = [(1, 2, 19), (1, 6, 15), (2, 3, 6), (2, 4, 10), 
         (2, 6, 22), (3, 4, 51), (3, 5, 14), (4, 8, 20),
         (4, 9, 42), (6, 7, 30)]
  
G.add_weighted_edges_from(edges)
nx.draw_networkx(G, with_labels = True)
S = degreeDiscountIC(G, 9, 0.0)
print(S)
S = degreeDiscountIC(G, 9, 0.2)
print(S)
S = degreeDiscountStar(G, 9, 0.3)
print(S)
S = degreeDiscountStar(G, 9, 0.4)
print(S)
S = degreeDiscountStar(G, 9, 0.7)
print(S)

z=degreeDiscountIC(graph, 10, 0.1)
print("Top 10 terror groups are:\n",'\n'.join([i_name[i] for i in z]))
viewPriorityNode(graph,z)

z=degreeDiscountStar(graph, 10, 0.1)
print("Top 10 terror groups are:\n",'\n'.join([i_name[i] for i in z]))
viewPriorityNode(graph,z)

def findDisimilarity(a,b):
  d1={}
  d2={}
  for i,j in enumerate(a):
    d1[j]=i
  for i,j in enumerate(b):
    d2[j]=i
  s=0
  for i in range(len(a)):
    s+=abs(d1[a[i]]-(-len(a) if a[i] not in d2 else d2[a[i]]))
  return s

#Testing
a=[1,2,3,4]
b=[1,3,4,2]
print(a,b)
print(findDisimilarity(a,b))
print(findDisimilarity(a,a))

S = degreeDiscountIC(graph, graph.number_of_nodes(), 0.0)
arr=[]
arr.append(findDisimilarity(S,S))
for i in range(1,11):
  arr.append(findDisimilarity(S,degreeDiscountIC(graph, graph.number_of_nodes(), i/10)))

x=[i/10 for i in range(0,11)]
plt.title("Effect of p on top K nodes")
plt.ylabel("Dissimilarities")
plt.xlabel("P value")
plt.plot(x,arr)

S = degreeDiscountStar(graph, graph.number_of_nodes(), 0.0)
arr=[]
arr.append(findDisimilarity(S,S))
for i in range(1,11):
  arr.append(findDisimilarity(S,degreeDiscountStar(graph, graph.number_of_nodes(), i/10)))

x=[i/10 for i in range(0,11)]
plt.title("Effect of p on top K nodes")
plt.ylabel("Dissimilarities")
plt.xlabel("P value")
plt.plot(x,arr)

"""R3"""

import numpy as np
import networkx as nx
from tqdm.autonotebook import tqdm
class LinearThreshold(object):
    def __init__(self, graph):
        self.graph = graph
        self.neighborhood_fn = self.graph.neighbors if isinstance(self.graph, nx.Graph) else self.graph.predecessors
    
    def sample_node_thresholds_mc(self, mc):
        self.sampled_thresholds = np.random.uniform(size=(mc, len(self.graph.nodes)))

    def sample_node_thresholds(self, mcount):
        for idx, node in enumerate(self.graph.nodes):
            self.graph.nodes[node]['threshold'] = self.sampled_thresholds[mcount][idx]

    def diffusion_iter(self):
        for node in self.graph.nodes:
            if self.graph.nodes[node]['is_active']:
                continue
            neighbors = self.neighborhood_fn(node)
            weights = sum(self.graph.edges[neighbor, node]['weight'] for neighbor in neighbors)
            if weights > self.graph.nodes[node]['threshold']:
                self.graph.nodes[node]['is_active'] = True

    def diffuse(self, act_nodes, mcount):
        self.sample_node_thresholds(mcount)
        nx.set_node_attributes(self.graph, False, name='is_active')

        for node in act_nodes:
            self.graph.nodes[node]['is_active'] = True

        prev_active_nodes = set()
        active_nodes = set()
        while True:
            self.diffusion_iter()
            prev_active_nodes = active_nodes
            active_nodes = set(i for i, v in self.graph.nodes(data=True) if v['is_active'])
            if active_nodes == prev_active_nodes:
                break
        self.graph.total_activated_nodes.append(len(active_nodes))

    def diffuse_mc(self, act_nodes, mc=50):
        self.sample_node_thresholds_mc(mc)
        self.graph.total_activated_nodes = []
        for mcount in range(mc):
            self.diffuse(act_nodes, mcount)
        return sum(self.graph.total_activated_nodes) / float(mc)

class IndependentCascade(object):
    def __init__(self, graph):
        self.graph = graph
        self.sampled_graph = graph.copy()
        self.edge_idx = {(u, v): i for i, (u, v) in enumerate(self.graph.edges())}
        self.reverse_edge_idx = {i: e for e, i in self.edge_idx.items()}
        self.prob_matrix = [self.graph.edges[self.reverse_edge_idx[i][0], self.reverse_edge_idx[i][1]]['prob'] for i in sorted(self.reverse_edge_idx.keys())]
    
    def sample_live_graph_mc(self, mc):
        edge_probs = {(u, v): d['prob'] for u, v, d in self.graph.edges().data()}
        probs = np.random.uniform(size=(mc, len(edge_probs)))
        self.sampled_graphs = []
        for p in probs:
            self.sampled_graphs.append(np.array([p > self.prob_matrix]).astype(np.int8))
        
    def sample_live_graph(self, mcount):
        removed_edges_idx = np.where(self.sampled_graphs[mcount] == 0)[1].tolist()
        removed_edges = [self.reverse_edge_idx[i] for i in removed_edges_idx]
        Gp = self.graph.copy()
        Gp.remove_edges_from(removed_edges)
        self.sampled_graph = Gp

    def diffusion_iter(self, act_nodes):
        new_act_nodes = set(act_nodes)
        for node in act_nodes:
            for node2 in nx.algorithms.bfs_tree(self.sampled_graph, node).nodes():
                new_act_nodes.add(node2)
        for node in new_act_nodes:
            self.sampled_graph.nodes[node]['is_active'] = True

    def diffuse(self, act_nodes, mcount):
        self.sample_live_graph(mcount)
        nx.set_node_attributes(self.sampled_graph, False, name='is_active')

        for node in act_nodes:
            self.sampled_graph.nodes[node]['is_active'] = True
        
        self.diffusion_iter(act_nodes)
        active_nodes = [n for n, v in self.sampled_graph.nodes.data() if v['is_active']]
        self.graph.total_activated_nodes.append(len(active_nodes))

    def diffuse_mc(self, act_nodes, mc=10):
        self.sample_live_graph_mc(mc)
        self.graph.total_activated_nodes = []
        for mcount in range(mc):
            self.diffuse(act_nodes, mcount)
        return sum(self.graph.total_activated_nodes) / float(mc)

def create_dist_for_graph_edges(graph):
    probs = [random.random() for _ in range(len(graph.edges))]
    for edge, p in zip(graph.edges, probs):
        graph.edges[edge]['prob'] = p
    return graph

graph=create_dist_for_graph_edges(graph)

import networkx as nx

def degree(graph, k):
    return [x for x, y in sorted(graph.degree, key=lambda x: x[1], reverse=True)[:k]]

def single_degree_discount(graph, k):
    graph=graph.copy()
    degree_count = dict(graph.degree)
    topk = []
    neighborhood_fn = graph.neighbors if isinstance(graph, nx.Graph) else graph.predecessors
    for _ in range(k):
        node = max(degree_count.items(), key=lambda x: x[1])[0]
        topk.append(node)
        for neighbor in neighborhood_fn(node):
            degree_count[neighbor] -= 1
        graph.remove_node(node)
        del degree_count[node]
    return topk

def top_k(graph, diffuse, k):
    expected_spread = []
    for node in graph.nodes:
        expected_spread.append([node, diffuse.diffuse_mc([node])])
    return [x for x, y in sorted(expected_spread, key=lambda x: x[1], reverse=True)[:k]]

def greedy(graph, diffuse, k):
	S = set()
	A = set(graph.nodes)
	while len(S) < k:
		node_diffusion = {}
		for node in A:
			S.add(node)
			node_diffusion[node] = diffuse.diffuse_mc(S)
			S.remove(node)
		max_node = max(node_diffusion.items(), key=lambda x: x[1])[0]
		S.add(max_node)
		A.remove(max_node)
	return S

import networkx as nx
G = nx.Graph()
  
edges = [(1, 2, 19), (1, 6, 15), (2, 3, 6), (2, 4, 10), 
         (2, 6, 22), (3, 4, 51), (3, 5, 14), (4, 8, 20),
         (4, 9, 42), (6, 7, 30)]
  
G.add_weighted_edges_from(edges)

import time
class time_taken:
  def __init__(self,model_name):
    self.start=time.time()
    self.model_name=model_name
  def print_time(self):
    print(self.model_name," Model took ",time.time()-self.start," secondes");
  def ret_time(self):
    return time.time()-self.start

def norm_weight(G):
  s=sum([i[2] for i in G.edges().data("weight")])
  for i in G.edges():
    G[i[0]][i[1]]["weight"]/=s
norm_weight(graph)
#G.edges().data("weight")

diffuse=LinearThreshold(graph)

a=time_taken("singel degree discount")
sd=single_degree_discount(graph,3)
a.print_time()

a=time_taken("top_k")
t_k=top_k(graph,diffuse,4)
a.print_time()

a=time_taken("greedy")
gr=greedy(graph,diffuse,4)
a.print_time()

print("\nsingle_degree_discount:\n",'\n'.join([i_name[i] for i in sd]))
print("\ntop_k:\n",'\n'.join([i_name[i] for i in t_k]))
print("\ngreedy:\n",'\n'.join([i_name[i] for i in gr]))

diffuse=IndependentCascade(graph)

a=time_taken("singel degree discount")
sd=single_degree_discount(graph,3)
a.print_time()

a=time_taken("top_k")
t_k=top_k(graph,diffuse,4)
a.print_time()

a=time_taken("greedy")
gr=greedy(graph,diffuse,4)
a.print_time()

print("\nsingle_degree_discount:\n",'\n'.join([i_name[i] for i in sd]))
print("\ntop_k:\n",'\n'.join([i_name[i] for i in t_k]))
print("\ngreedy:\n",'\n'.join([i_name[i] for i in gr]))

"""Trend in time"""

time1=[]
for i in range(10,60,10):
  a=time_taken("greedy")
  sd=single_degree_discount(graph,i)
  time1.append(a.ret_time())

time2=[]
for i in range(10,60,10):
  diffuse=IndependentCascade(graph)
  a=time_taken("single degree discount")
  t_k=top_k(graph,diffuse,i)
  time2.append(a.ret_time())

time3=[]
for i in range(10,60,10):
  diffuse=IndependentCascade(graph)
  a=time_taken("top-k")
  gr=greedy(graph,diffuse,i)
  time3.append(a.ret_time())

from matplotlib import pyplot as plt
x=[i/10 for i in range(10,60,10)]
plt.title("k node v/s time")
plt.ylabel("time in sec")
plt.xlabel("k nodes")
plt.plot(x,time1,label='degree discount')
plt.plot(x,time2,label='top_k')
x=[i/10 for i in range(10,40,10)]
plt.plot(x,time3,label='greedy')
plt.show()

time2=[]
for i in range(10,60,10):
  diffuse=LinearThreshold(graph)
  a=time_taken("single degree discount")
  t_k=top_k(graph,diffuse,i)
  time2.append(a.ret_time())

time3=[]
for i in range(10,40,10):
  diffuse=LinearThreshold(graph)
  a=time_taken("top-k")
  gr=greedy(graph,diffuse,i)
  time3.append(a.ret_time())

time3

from matplotlib import pyplot as plt
x=[i/10 for i in range(10,60,10)]
plt.title("k node v/s time")
plt.ylabel("time in sec")
plt.xlabel("k nodes")
plt.plot(x,time1,label='degree discount')
plt.plot(x,time2,label='top_k')
x=[i/10 for i in range(10,20,10)]
plt.plot(x,time3,label='greedy')
plt.show()